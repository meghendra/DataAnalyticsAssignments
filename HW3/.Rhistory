demo()
q()
load(iris.dat)
h = [100, 120, 150, 80, 50]
h = (100, 120, 150, 80, 50);
h <- (100, 120, 150, 80, 50);
h <- (100, 120, 150, 80, 50)
h <- [100, 120, 150, 80, 50]
h <- [100 120 150 80 50]
h <- (100 120 150 80 50)
h <- c(100, 120, 150, 80, 50)
y <- c(1,3,5,3,6)
mean(h)
mean(y)
cov(h,y);
y <- c(1,2,3,4,5)
h <- c(50, 80, 100, 120, 150)
cov(h,y);
h1 <- c(-50, -30, 0, 20, 50)
cov(h1,y);
cov(y,h);
cov(y,h1);
h1 <- c(-50, -20, 0, 20, 50)
cov(y,h1);
mean(h1)
x <- c(2.5, 3.6, 10, 12, 24.3)
mean(x)
x - mean(x);
sum (x - mean(x));
z = x - mean(x);
z
sum(z)
x <- c(2, 3, 10, 12, 24)
z = x - mean(x);
sum(z)
mean(h1)
mean(z)
h2 <- h1./2.4
h2 <- h1 / 2.4
h2
h
cov(y,h2);
cov(y,h);
60 / 2.4
mean(h)
mean(h)/2.4
mean(h2)
mean(h1)
h2 <- h / 2.4
mean(h2)
cov(y,h);
cov(y,h2);
60 / 2.4
tdMatrix <- matrix(data=[0,2,3,4,0,4,0,5,0,3,2,0,4,1,3,0,3,1,5,1,1,3,5,0,2,3,2,0,0,5,3,9],nrow = 4, ncol = 8)
tdMatrix <- matrix(data= (0,2,3,4,0,4,0,5,0,3,2,0,4,1,3,0,3,1,5,1,1,3,5,0,2,3,2,0,0,5,3,9),nrow = 4, ncol = 8)
tdMatrix <- matrix(data= (0 2 3 4 0 4 0 5 0 3 2 0 4 1 3 0 3 1 5 1 1 3 5 0 2 3 2 0 0 5 3 9),nrow = 4, ncol = 8)
tdMatrix <- matrix(data= 0 2 3 4 0 4 0 5 0 3 2 0 4 1 3 0 3 1 5 1 1 3 5 0 2 3 2 0 0 5 3 9,nrow = 4, ncol = 8)
tdMatrix <- rbind( c( 0 2 3 4 0 4 0 5), c(0 3 2 0 4 1 3 0), c(3 1 5 1 1 3 5 0), c(2 3 2 0 0 5 3 9))
tdMatrix <- rbind( c( 0, 2, 3, 4, 0, 4, 0, 5), c(0, 3, 2, 0, 4, 1, 3, 0), c(3, 1, 5, 1, 1, 3, 5, 0), c(2, 3, 2, 0, 0, 5, 3, 9))
tdMatrix
tf <- sort(rowSums(as.matrix(tdMatrix)), decreasing = TRUE)
tf
sum(tdMatrix[1:,:])
sum(tdMatrix(1:,:)
sum(tdMatrix(1:,:))
sum(tdMatrix(1:;:))
sum(tdMatrix(1:1,:))
colsum(tdMatrix)
colSums(tdMatrix)
rowSums(tdMatrix)
colSums(tdMatrix)
nrow(tdMatrix)
id=function(col){sum(!col==0)}
id
idf <- log(nrow(tdMatrix)/apply(tdMatrix, 2, id))
idf
w <- tdMatrix .* idf;
w <- tdMatrix *as.vector(idf);
w
w <- tdMatrix *idf;
w
w <- idf * tdMatrix;
w
sweep(tdMatrix,MARGIN = 2,idf,'*')
w <- sweep(tdMatrix,MARGIN = 2,idf,'*')
w
q <- c(0,0,0,1,0,0,0,1)
q
wq <- sweep(q,MARGIN = 2,idf,'*')
wq <- q *as.vector(idf);
wq
cosine(w,wq)
import lsa;
install.packages("lsa")
import lsa;
cosine(w,wq)
library(lsa)
library('lsa')
cosine(w,wq)
cosine(w(1,),wq)
cosine(w[1,],wq)
cosine(w[2,],wq)
cosine(w[3,],wq)
cosine(w[4,],wq)
cosine(w[5,],wq)
cosine(w[1,],q)
cosine(w[2,],q)
cosine(w[3,],q)
cosine(w[4,],q)
x= rbind(c(0,0),c(5,12))
x
dist(x,method = 'minkowski',p = 1)
dist(x,method = 'minkowski',p = 2)
dist(x,method = 'minkowski',p = 4)
dist(x,method = 'minkowski',p = 8)
dist(x,method = 'minkowski',p = 10)
dist(x,method = 'minkowski',p = 100)
dist(x,method = 'minkowski',p = 100000)
dist(x,method = 'minkowski',p = 10000)
dist(x,method = 'minkowski',p = 1000)
dist(x,method = 'minkowski',p = 100)
dist(x,method = 'minkowski',p = 500)
dist(x,method = 'minkowski',p = 200)
dist(x,method = 'minkowski',p = 250)
dist(x,method = 'minkowski',p = 290)
dist(x,method = 'minkowski',p = 280)
dist(x,method = 'minkowski',p = 285)
dist(x,method = 'minkowski',p = 289)
dist(x,method = 'minkowski',p = 286)
dist(x,method = 'minkowski',p = 285)
dist(x,method = 'minkowski',p = 286)
dist(x,method = 'minkowski',p = 1)
dist(x,method = 'minkowski',p = 2)
dist(x,method = 'minkowski',p = 3)
dist(x,method = 'minkowski',p = 4)
dist(x,method = 'minkowski',p = 5)
dist(x,method = 'minkowski',p = 8)
#knn
train <- read.csv("train.csv")
test <- read.csv("test.csv")
head(train)
head(test)
normalize <- function(x) {
return ( (x - min(x)) / (max(x) - min(x)))
}
train_n <- as.data.frame(lapply(train[,c(1,2,3)], normalize))
# test.csv has an additional column ID which is
# useless hence discarded
test_n <- as.data.frame(lapply(test[,c(2,3,4)], normalize))
train_labels <- as.data.frame(train[,c(4)])
test_labels <- as.data.frame(test[,c(5)])
require(class)
m1 <- knn(train=train_n, test=test_n, cl=train_labels[,1], k=3, prob = TRUE)
m1
# plot the contingency table
table(test_labels[,1],m1)
setwd('Downloads/GoogleDrive/DAFall16/HW3/')
## utility functions used in the rest of the code
# normalize the data with min max
normalize <- function(x) {
nx <- ((x - min(x)) / (max(x) - min(x)))
return(nx)
}
# normalize the data with z-score
zScoreNormalize <- function(x) {
nx <- (x - mean(x)) / sd(x)
return(nx)
}
# find euclidean distance between test and training data
distance <- function(x, y) {
dxy <- sqrt(sum(( x - y )^2))
return(dxy)
}
# find k nearest neighbors by distance
getKnnByDistance <- function(dxy, k) {
knnbd <- order(dxy) [1:k]
return (knnbd)
}
# find weights for the votes of k nearest neighbors by distance
getKnnWeightsByDistance <- function(dxy, k) {
knnbd <- order(dxy) [1:k]
knnwbd <- 1/(dxy[knnbd])^2
return (knnwbd)
}
# find the majority vote / weighted majority vote for each label
findVotes <- function(neighborIds, weightVector=rep(1, length(neighborIds))) {
result <- matrix(0, nrow=2, ncol = 3)
knnLabels <- train_labels[neighborIds,1]
for (i in 1 : length(knnLabels))
{
if (knnLabels[i] == 0) {
result[1,] <- c(0, (sum (1, result[1,2])) * weightVector[i], 0)
}
if (knnLabels[i] == 1) {
result[2,] <- c(1, (sum (1, result[2,2])) * weightVector[i], 0)
}
}
result[,3] <- result[,2] / sum(result[,2])
return(result)
}
#load data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
head(train)
head(test)
# test.csv has an additional column ID which is useless hence discarded
## clean and normalize the data by min-max normalization
# train_n <- as.data.frame(lapply(train[,c(1,2,3)], normalize))
# test_n <- as.data.frame(lapply(test[,c(2,3,4)], normalize))
# train_labels <- as.data.frame(train[,c(4)])
# test_labels <- as.data.frame(test[,c(5)])
## OR
## normalize the data using z-score normalization
# train_n <- as.data.frame(lapply(train[,c(1,2,3)], zScoreNormalize))
# test_n <- as.data.frame(lapply(test[,c(2,3,4)], zScoreNormalize))
# train_labels <- as.data.frame(train[,c(4)])
# test_labels <- as.data.frame(test[,c(5)])
# OR
## dont normalize the data
train_n <- as.data.frame(train[,c(1,2,3)])
test_n <- as.data.frame(test[,c(2,3,4)])
train_labels <- as.data.frame(train[,c(4)])
test_labels <- as.data.frame(test[,c(5)])
## compute distance matrix d
d <- matrix(NA, nrow = nrow(test_n), ncol = nrow(train_n))
for (i in 1:nrow(test_n)) {
d[i,] = apply(train_n[,1:3], 1, distance, x = test_n[i,1:3])
}
d <- t(d)
##############################################
## For unweighted kNN with k=3
## Compute ids of k-NN with k = 3
knns <- apply(d, 2, getKnnByDistance, k=3)
predictedLabelAggr <- vector()
predictedProbAggr <- vector()
for (i in 1:ncol(knns)){
votes <- findVotes(knns[,i])
predictedLabelAggr[i] <- votes[which.max(votes[,3]), 1]
predictedProbAggr[i] <- votes[which.max(votes[,3]), 3]
}
result <- cbind(predictedLabelAggr, predictedProbAggr)
# write confusion matrix
table(predictedLabelAggr, test_labels[,1])
###############################################
## For weighted k-NN with k = 3 and w=(1/d^2)
## determine the weightvectors of the kNNs
weightVectors <- apply(d, 2, getKnnWeightsByDistance, k=3)
predictedLabelAggr <- vector()
predictedProbAggr <- vector()
for (i in 1:ncol(knns)){
votes <- findVotes(knns[,i], weightVector = weightVectors[,i])
predictedLabelAggr[i] <- votes[which.max(votes[,3]), 1]
predictedProbAggr[i] <- votes[which.max(votes[,3]), 3]
}
resultWeighted <- cbind(predictedLabelAggr, predictedProbAggr)
# write confusion matrix
table(predictedLabelAggr, test_labels[,1])
test_labels
